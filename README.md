# Propz - Technical Challenge 

Say there is AWS EMR cluster with some Spark job. It computes amount of purchases for middle-aged customers by each day of month.
How would you modify the code ( make it better from performance and style standpoints ) ?
Please describe in details all of your modifications ( any modified/added line ).

1. [Description](README.md#Description)
2. [Input](README.md#input)
3. [Output](README.md#output)
4. [Execution](README.md#execution)

# Description

I've decided to refactor the entire code because I do believe that loop statements are an anti-pattern in distributed computing. 

These changes were made to make the code easier to read and maintain. Beyond that, I did some improvements in data loading, thus I just loading data needed. For really large datasets, this approach may prevent serious performance issues.

Before making any changes, I measured the improvements using the explain method for every code line.

# Input
Input files are located at propz/raw_data/ directory and contains 1k rows both. These files were generated by a data generator online.
- propz/raw_data/customers.csv
- propz/raw_data/transactions.csv

# Output
The output will be located at propz/raw_data/results/ directory filled with several parquet files partitioned by customers_name and day.

# Execution
To run the code, follow the steps ahead.

    1.) clone the repository with below command
      git clone https://github.com/daniel-dqsdatalabs/propz.git
      
    2.) Go to the script directory
       cd propz
       
    3.) initialize spark
        spark-shell
        
    4.) initialize spark
        load: script.scala
        
    5.) run the script and see results
        Propz.main(Array(""))
